{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import functools\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot detect physical GPU device in TF\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if not gpu_devices:\n",
    "  print('Cannot detect physical GPU device in TF')\n",
    "# tf.config.set_logical_device_configuration(\n",
    "#     gpu_devices[0], \n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=5120),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=5120)])\n",
    "# tf.config.list_logical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting uop tensorboard \n",
    " \n",
    "# import os \n",
    "# import shutil\n",
    "\n",
    "# logdir = \"C:\\Users\\sidha\\Stevens\\AAI800 - Special Projects in AI\\Project Execution\\tensorboard\"\n",
    "# if os.path.exists(logdir):\n",
    "#     shutil.rmtree(logdir)\n",
    "\n",
    "# summary_writer = tf.summary.create_file_writer(logdir)\n",
    "# state = iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "hello_world()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_client_dataset_size = 500\n",
    "MAX_CLIENT_DATASET_SIZE = int(input(\"Enter Max Client Datset Size: \") or default_client_dataset_size)\n",
    "\n",
    "default_client_epochs_per_round = 1\n",
    "CLIENT_EPOCHS_PER_ROUND = int(input(\"Enter Client Epochs pe round: \") or default_client_epochs_per_round)\n",
    "\n",
    "default_client_batch_size = 20\n",
    "CLIENT_BATCH_SIZE = int(input(\"Enter Client Batch Size: \") or default_client_batch_size)\n",
    "\n",
    "default_client_test_batch_size = 100\n",
    "TEST_BATCH_SIZE = int(input(\"Enter Test Batch Size: \") or default_client_test_batch_size)\n",
    "\n",
    "NUM_CLIENTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train, cifar_test = tff.simulation.datasets.cifar100.load_data(cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('coarse_label', TensorSpec(shape=(), dtype=tf.int64, name=None)),\n",
       "             ('image',\n",
       "              TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None)),\n",
       "             ('label', TensorSpec(shape=(), dtype=tf.int64, name=None))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_train.element_type_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dataset = cifar_train.create_tf_dataset_for_client(cifar_train.client_ids[0])\n",
    "example_element= next(iter(example_dataset))\n",
    "example_element['label'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 3), dtype=uint8, numpy=\n",
       "array([[[ 89,  71,  73],\n",
       "        [120,  98, 106],\n",
       "        [137, 113, 116],\n",
       "        ...,\n",
       "        [133, 105, 102],\n",
       "        [131, 104,  98],\n",
       "        [142, 121, 107]],\n",
       "\n",
       "       [[ 94,  76,  75],\n",
       "        [123, 101, 105],\n",
       "        [169, 144, 145],\n",
       "        ...,\n",
       "        [152, 123, 120],\n",
       "        [133, 107, 101],\n",
       "        [168, 146, 137]],\n",
       "\n",
       "       [[133, 113, 110],\n",
       "        [151, 128, 130],\n",
       "        [147, 121, 120],\n",
       "        ...,\n",
       "        [167, 139, 133],\n",
       "        [164, 138, 130],\n",
       "        [197, 174, 168]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 87,  69,  74],\n",
       "        [102,  82,  85],\n",
       "        [ 97,  73,  75],\n",
       "        ...,\n",
       "        [175, 158, 152],\n",
       "        [143, 124, 121],\n",
       "        [158, 134, 130]],\n",
       "\n",
       "       [[ 87,  68,  69],\n",
       "        [100,  79,  79],\n",
       "        [110,  86,  84],\n",
       "        ...,\n",
       "        [148, 131, 123],\n",
       "        [160, 143, 143],\n",
       "        [172, 155, 157]],\n",
       "\n",
       "       [[ 93,  74,  71],\n",
       "        [157, 136, 131],\n",
       "        [170, 145, 139],\n",
       "        ...,\n",
       "        [164, 147, 136],\n",
       "        [164, 149, 151],\n",
       "        [189, 178, 185]]], dtype=uint8)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_element['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reshape_cifar_element(element):\n",
    "#   return (tf.expand_dims(element['image'], axis=-1), element['coarse_label'])\n",
    "def reshape_cifar_element(element):\n",
    " \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    " x=tf.reshape(element['image'], [-1, 3072])\n",
    " x=tf.cast(x,dtype=tf.float32)\n",
    " x /= 255\n",
    " y=tf.reshape(element['coarse_label'], [-1, 1])\n",
    " return collections.OrderedDict( \n",
    "    x=(tf.cast(tf.reshape(element['image'], [-1, 3072]),dtype=tf.float32))/255,\n",
    "    y=tf.reshape(element['label'], [-1, 1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "  return (dataset\n",
    "          .shuffle(buffer_size=MAX_CLIENT_DATASET_SIZE)\n",
    "          .repeat(CLIENT_EPOCHS_PER_ROUND)\n",
    "          .batch(CLIENT_BATCH_SIZE, drop_remainder=False)\n",
    "          .map(reshape_cifar_element)\n",
    "          .prefetch(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('x',\n",
       "              array([[0.42745098, 0.49803922, 0.7372549 , ..., 0.5529412 , 0.6901961 ,\n",
       "                      0.9411765 ],\n",
       "                     [0.04705882, 0.1254902 , 0.4627451 , ..., 0.2       , 0.27058825,\n",
       "                      0.39607844],\n",
       "                     [0.99607843, 0.99607843, 0.99607843, ..., 0.9843137 , 0.9843137 ,\n",
       "                      0.9843137 ],\n",
       "                     ...,\n",
       "                     [0.49411765, 0.54509807, 0.5529412 , ..., 0.59607846, 0.5058824 ,\n",
       "                      0.4862745 ],\n",
       "                     [0.3137255 , 0.54509807, 0.88235295, ..., 0.28235295, 0.39215687,\n",
       "                      0.18431373],\n",
       "                     [0.2509804 , 0.2509804 , 0.2509804 , ..., 0.3254902 , 0.3254902 ,\n",
       "                      0.3254902 ]], dtype=float32)),\n",
       "             ('y',\n",
       "              array([[23],\n",
       "                     [23],\n",
       "                     [29],\n",
       "                     [71],\n",
       "                     [93],\n",
       "                     [51],\n",
       "                     [93],\n",
       "                     [75],\n",
       "                     [29],\n",
       "                     [33],\n",
       "                     [49],\n",
       "                     [34],\n",
       "                     [49],\n",
       "                     [60],\n",
       "                     [15],\n",
       "                     [33],\n",
       "                     [93],\n",
       "                     [44],\n",
       "                     [49],\n",
       "                     [99]], dtype=int64))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example_dataset = preprocess_dataset(example_dataset)\n",
    "\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
    "                                     next(iter(preprocessed_example_dataset)))\n",
    "\n",
    "sample_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_federated_data(client_data, client_ids):\n",
    "  return [\n",
    "      preprocess_dataset(client_data.create_tf_dataset_for_client(x))\n",
    "      for x in client_ids\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 2\n",
      "First dataset: <PrefetchDataset shapes: OrderedDict([(x, (None, 3072)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.int64)])>\n"
     ]
    }
   ],
   "source": [
    "sample_clients = cifar_train.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "federated_train_data = make_federated_data(cifar_train, sample_clients)\n",
    "\n",
    "print(f'Number of client datasets: {len(federated_train_data)}')\n",
    "print(f'First dataset: {federated_train_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_train_federated = cifar_train.preprocess(preprocess_train_dataset)\n",
    "# cifar_train_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_original_fedavg_cnn_model(cache_dir=None):\n",
    "#   data_format = 'channels_last'\n",
    "\n",
    "#   max_pool = functools.partial(\n",
    "#       tf.keras.layers.MaxPooling2D,\n",
    "#       pool_size=(2, 2),\n",
    "#       padding='same',\n",
    "#       data_format=data_format)\n",
    "#   conv2d = functools.partial(\n",
    "#       tf.keras.layers.Conv2D,\n",
    "#       kernel_size=5,\n",
    "#       padding='same',\n",
    "#       data_format=data_format,\n",
    "#       activation=tf.nn.relu)\n",
    "#   model = tf.keras.applications.resnet.ResNet101(include_top=False,\n",
    "#                                                  weights='imagenet',\n",
    "#                                                  input_tensor=None,\n",
    "#                                                  input_shape=(32,32,3),\n",
    "#                                                  pooling=max,\n",
    "#                                                  classes=20,\n",
    "#                                                  )\n",
    "  \n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "BatchNormalization()\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "\n",
    "\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model=create_original_fedavg_cnn_model()\n",
    "Model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 936,580\n",
      "Trainable params: 936,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "  # We _must_ create a new model here, and _not_ capture it from an external\n",
    "  # scope. TFF will call this within different graph contexts.\n",
    "  return tff.learning.from_keras_model(\n",
    "      Model,\n",
    "      input_spec= preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100Variables = collections.namedtuple('cifar100Variables','weights bias num_examples loss_sum accuracy_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cifar100_variables():\n",
    "    return cifar100Variables(\n",
    "      weights=tf.Variable(\n",
    "          lambda: tf.zeros(dtype=tf.float32, shape=(3072,100)),\n",
    "          name='weights',\n",
    "          trainable=True),\n",
    "      bias=tf.Variable(\n",
    "          lambda: tf.zeros(dtype=tf.float32, shape=(100)),\n",
    "          name='bias',\n",
    "          trainable=True),\n",
    "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
    "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
    "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_model(variables,x):\n",
    "    return tf.nn.softmax(tf.matmul(x, variables.weights) + variables.bias)\n",
    "    # tff.learning.from_keras_model(\n",
    "    #   keras_model=Model(),\n",
    "    #   input_spec=input_spec,\n",
    "    #   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    #   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "      \n",
    "def cifar100_forward_pass(variables, batch):\n",
    "  y = batch_model(variables, batch['x'])\n",
    "  predictions = tf.cast(tf.argmax(y, 1), tf.int64)\n",
    "\n",
    "  flat_labels = tf.reshape(batch['y'], [-1])\n",
    "  loss = -tf.reduce_mean(\n",
    "      tf.reduce_sum(tf.one_hot(flat_labels, 100) * tf.math.log(y), axis=[1]))\n",
    "  accuracy = tf.reduce_mean(\n",
    "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
    "\n",
    "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
    "\n",
    "  variables.num_examples.assign_add(num_examples)\n",
    "  variables.loss_sum.assign_add(loss * num_examples)\n",
    "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
    "\n",
    "  return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_unfinalized_metrics(variables):\n",
    "  return collections.OrderedDict(\n",
    "      num_examples=[variables.num_examples],\n",
    "      loss=[variables.loss_sum, variables.num_examples],\n",
    "      accuracy=[variables.accuracy_sum, variables.num_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_finalizers():\n",
    "  return collections.OrderedDict(\n",
    "      num_examples=tf.function(func=lambda x: x[0]),\n",
    "      loss=tf.function(func=lambda x: x[0] / x[1]),\n",
    "      accuracy=tf.function(func=lambda x: x[0] / x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_mnist_metrics(variables):\n",
    "  return collections.OrderedDict(\n",
    "      num_examples=variables.num_examples,\n",
    "      loss=variables.loss_sum / variables.num_examples,\n",
    "      accuracy=variables.accuracy_sum / variables.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation\n",
    "def aggregate_mnist_metrics_across_clients(metrics):\n",
    "  return collections.OrderedDict(\n",
    "      num_examples=tff.federated_sum(metrics.num_examples),\n",
    "      loss=tff.federated_mean(metrics.loss, metrics.num_examples),\n",
    "      accuracy=tff.federated_mean(metrics.accuracy, metrics.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, OrderedDict\n",
    "\n",
    "class cifar100Model(tff.learning.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._variables = create_cifar100_variables()\n",
    "\n",
    "  @property\n",
    "  def trainable_variables(self):\n",
    "    return [self._variables.weights, self._variables.bias]\n",
    "\n",
    "  @property\n",
    "  def non_trainable_variables(self):\n",
    "    return []\n",
    "\n",
    "  @property\n",
    "  def local_variables(self):\n",
    "    return [\n",
    "        self._variables.num_examples, self._variables.loss_sum,\n",
    "        self._variables.accuracy_sum\n",
    "    ]\n",
    "\n",
    "  @property\n",
    "  def input_spec(self):\n",
    "    return collections.OrderedDict(\n",
    "        x=tf.TensorSpec([None, 3072], tf.float32),\n",
    "        y=tf.TensorSpec([None, 1], tf.int64))\n",
    "\n",
    "  @tf.function\n",
    "  def predict_on_batch(self, x, training=True):\n",
    "    del training\n",
    "    return batch_model(self._variables, x)\n",
    "    \n",
    "  @tf.function\n",
    "  def forward_pass(self, batch, training=True):\n",
    "    del training\n",
    "    loss, predictions = cifar100_forward_pass(self._variables, batch)\n",
    "    num_examples = tf.shape(batch['x'])[0]\n",
    "    return tff.learning.BatchOutput(\n",
    "        loss=loss, predictions=predictions, num_examples=num_examples)\n",
    "        \n",
    "  @tf.function\n",
    "  def report_local_outputs(self):\n",
    "    return get_local_mnist_metrics(self._variables)\n",
    "\n",
    "  @property\n",
    "  def federated_output_computation(self):\n",
    "    return aggregate_mnist_metrics_across_clients\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def report_local_unfinalized_metrics(\n",
    "      self) -> OrderedDict[str, List[tf.Tensor]]:\n",
    "    \"\"\"Creates an `OrderedDict` of metric names to unfinalized values.\"\"\"\n",
    "    return get_local_unfinalized_metrics(self._variables)\n",
    "\n",
    "  def metric_finalizers(\n",
    "      self) -> OrderedDict[str, Callable[[List[tf.Tensor]], tf.Tensor]]:\n",
    "    \"\"\"Creates an `OrderedDict` of metric names to finalizers.\"\"\"\n",
    "    return get_metric_finalizers()\n",
    "\n",
    "  @tf.function\n",
    "  def reset_metrics(self):\n",
    "    \"\"\"Resets metrics variables to initial value.\"\"\"\n",
    "    for var in self.local_variables:\n",
    "      var.assign(tf.zeros_like(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model \n",
    "federated_averaging = tff.learning.(\n",
    "    model_fn=cifar100Model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "    # server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=5.0)\n",
    "    # use_experimental_simulation_loop=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = federated_averaging.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.9105146), ('accuracy', 0.075)]))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state, metrics = federated_averaging.next(state, federated_train_data)\n",
    "    # result = iterative_process.next(state, federated_train_data)\n",
    "    # state = result.state\n",
    "    # metrics = result.metrics\n",
    "print('round  1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.5689206), ('accuracy', 0.095)]))])\n",
      "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.5829897), ('accuracy', 0.105)]))])\n",
      "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.2940521), ('accuracy', 0.11)]))])\n",
      "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.0494468), ('accuracy', 0.15)]))])\n",
      "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 2.9852643), ('accuracy', 0.17)]))])\n",
      "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.0124512), ('accuracy', 0.16)]))])\n",
      "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 2.8435788), ('accuracy', 0.185)]))])\n",
      "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 3.066428), ('accuracy', 0.17)]))])\n",
      "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('num_examples', 200.0), ('loss', 2.7823591), ('accuracy', 0.205)]))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for round_num in range(2, 11):\n",
    "    state, metrics = federated_averaging.next(state, federated_train_data)\n",
    "        # result = iterative_process.next(state, federated_train_data)\n",
    "        # state = result.state\n",
    "        # metrics = result.metrics\n",
    "    print('round {:2d}, metrics={}'.format(round_num,metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%##############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<\n",
      "  server_model_weights=<\n",
      "    trainable=<\n",
      "      float32[3072,100],\n",
      "      float32[100]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >@SERVER,\n",
      "  federated_dataset={<\n",
      "    x=float32[?,3072],\n",
      "    y=int64[?,1]\n",
      "  >*}@CLIENTS\n",
      "> -> <\n",
      "  num_examples=float32@SERVER,\n",
      "  loss=float32@SERVER,\n",
      "  accuracy=float32@SERVER\n",
      ">)\n"
     ]
    }
   ],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(cifar100Model)\n",
    "print(evaluation.type_signature.formatted_representation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IterativeProcess' object has no attribute 'get_model_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21100\\2492029212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfederated_averaging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfederated_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'IterativeProcess' object has no attribute 'get_model_weights'"
     ]
    }
   ],
   "source": [
    "model_weights = federated_averaging.get_model_weights(state)\n",
    "train_metrics = evaluation(model_weights, federated_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_federated.python.learning.model_utils.ModelWeights"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights = tff.learning.ModelWeights(trainable=state.model,non_trainable=state.model)\n",
    "# train_metrics = evaluation(model_weights, federated_train_data)\n",
    "type(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelWeights(trainable=ModelWeights(trainable=[array([[ 0.00039498, -0.00067911, -0.00067911, ..., -0.00067911,\n",
       "        -0.00067911,  0.00221748],\n",
       "       [ 0.00033089, -0.00074313, -0.00074313, ..., -0.00074313,\n",
       "        -0.00074313,  0.00134677],\n",
       "       [-0.00044445, -0.00071834, -0.00071834, ..., -0.00071834,\n",
       "        -0.00071834,  0.00116415],\n",
       "       ...,\n",
       "       [ 0.0007081 , -0.00077924, -0.00077924, ..., -0.00077924,\n",
       "        -0.00077924,  0.00153356],\n",
       "       [ 0.00077423, -0.00078516, -0.00078516, ..., -0.00078516,\n",
       "        -0.00078516,  0.00098196],\n",
       "       [ 0.00092239, -0.00070942, -0.00070942, ..., -0.00070942,\n",
       "        -0.00070942,  0.00107656]], dtype=float32), array([ 0.00099185, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00034812,\n",
       "        0.0024676 , -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "        0.00122479,  0.00070398, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.00120148, -0.00104423,  0.00055036, -0.00186921,\n",
       "       -0.00186921, -0.00186921,  0.00451404, -0.00337521, -0.00155561,\n",
       "       -0.00186921, -0.00186921, -0.00186921,  0.01981477,  0.01456934,\n",
       "       -0.00186921, -0.00186921, -0.00186921,  0.00065316,  0.00097712,\n",
       "        0.00112643, -0.00186921, -0.00186921, -0.00186921,  0.00498318,\n",
       "        0.00201943, -0.00186921, -0.00186921, -0.00186921,  0.00051163,\n",
       "       -0.00186921,  0.00272718, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00115279,  0.00088912, -0.00186921,  0.01899637,  0.01598671,\n",
       "       -0.00186921,  0.01156697, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.0012366 , -0.00186921, -0.00186921, -0.00186921,\n",
       "        0.01178883, -0.00186921,  0.0032976 , -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.00036016, -0.00186921,  0.00130699, -0.00186921,\n",
       "       -0.00186921, -0.00186921,  0.00145865,  0.00584988, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921,  0.00093907],\n",
       "      dtype=float32)], non_trainable=[]), non_trainable=ModelWeights(trainable=[array([[ 0.00039498, -0.00067911, -0.00067911, ..., -0.00067911,\n",
       "        -0.00067911,  0.00221748],\n",
       "       [ 0.00033089, -0.00074313, -0.00074313, ..., -0.00074313,\n",
       "        -0.00074313,  0.00134677],\n",
       "       [-0.00044445, -0.00071834, -0.00071834, ..., -0.00071834,\n",
       "        -0.00071834,  0.00116415],\n",
       "       ...,\n",
       "       [ 0.0007081 , -0.00077924, -0.00077924, ..., -0.00077924,\n",
       "        -0.00077924,  0.00153356],\n",
       "       [ 0.00077423, -0.00078516, -0.00078516, ..., -0.00078516,\n",
       "        -0.00078516,  0.00098196],\n",
       "       [ 0.00092239, -0.00070942, -0.00070942, ..., -0.00070942,\n",
       "        -0.00070942,  0.00107656]], dtype=float32), array([ 0.00099185, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00034812,\n",
       "        0.0024676 , -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "        0.00122479,  0.00070398, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.00120148, -0.00104423,  0.00055036, -0.00186921,\n",
       "       -0.00186921, -0.00186921,  0.00451404, -0.00337521, -0.00155561,\n",
       "       -0.00186921, -0.00186921, -0.00186921,  0.01981477,  0.01456934,\n",
       "       -0.00186921, -0.00186921, -0.00186921,  0.00065316,  0.00097712,\n",
       "        0.00112643, -0.00186921, -0.00186921, -0.00186921,  0.00498318,\n",
       "        0.00201943, -0.00186921, -0.00186921, -0.00186921,  0.00051163,\n",
       "       -0.00186921,  0.00272718, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00115279,  0.00088912, -0.00186921,  0.01899637,  0.01598671,\n",
       "       -0.00186921,  0.01156697, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.0012366 , -0.00186921, -0.00186921, -0.00186921,\n",
       "        0.01178883, -0.00186921,  0.0032976 , -0.00186921, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921, -0.00186921,\n",
       "       -0.00186921,  0.00036016, -0.00186921,  0.00130699, -0.00186921,\n",
       "       -0.00186921, -0.00186921,  0.00145865,  0.00584988, -0.00186921,\n",
       "       -0.00186921, -0.00186921, -0.00186921, -0.00186921,  0.00093907],\n",
       "      dtype=float32)], non_trainable=[]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_weights_2 = tf.TensorSpec(model_weights)\n",
    "train_metrics = evaluation(model_weights.trainable, federated_train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef392b7373726638f25c5b5a9ae7a2125364017db5464905a5bcdac062fc1c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
